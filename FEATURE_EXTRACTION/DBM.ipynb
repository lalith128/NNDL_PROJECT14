{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 12:26:42.071800: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743683202.121446    2005 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743683202.134914    2005 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743683202.236858    2005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743683202.236975    2005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743683202.236977    2005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743683202.236979    2005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-03 12:26:42.254824: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2005/2678278497.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv(\"../DATA/train.csv\")\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../DATA/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 17, 0, '6', '17', '0', 'Protocol'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Protocol'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Protocol'] = pd.to_numeric(train['Protocol'], errors='coerce') \n",
    "train = train.dropna(subset=['Protocol']) \n",
    "train['Protocol'] = train['Protocol'].astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 17,  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Protocol'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Dst Port', 'Protocol', 'Flow Duration', 'Fwd Pkt Len Max',\n",
       "       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max',\n",
       "       'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd IAT Tot', 'Bwd IAT Mean',\n",
       "       'Bwd IAT Std', 'Fwd PSH Flags', 'RST Flag Cnt', 'PSH Flag Cnt',\n",
       "       'ACK Flag Cnt', 'Down/Up Ratio', 'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(train.columns) \n",
    "columns = [col for col in columns if col not in ['Fwd PSH Flags', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'Protocol', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.drop(columns=['Label'])\n",
    "y = train['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[columns] = x[columns].apply(pd.to_numeric, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=['Fwd PSH Flags', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt']\n",
    "x[f] = x[f].apply(pd.to_numeric, errors='coerce')\n",
    "x = x.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = MinMaxScaler()\n",
    "x[columns] = sc.fit_transform(x[columns])  \n",
    "joblib.dump(sc, 'scaler.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRBM(tf.keras.Model):\n",
    "    def __init__(self, inp_dim, hid_dim, lr=0.01, momentum=0.9, use_fp16=False):\n",
    "        super(FastRBM, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.inp_dim = inp_dim\n",
    "        self.momentum = momentum\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.mixed_precision_dtype = tf.float16 if use_fp16 else tf.float32\n",
    "        \n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "        self.W = tf.Variable(initializer([inp_dim, hid_dim]), dtype=tf.float32)\n",
    "        self.bh = tf.Variable(tf.zeros([hid_dim]), dtype=tf.float32)\n",
    "        self.bv = tf.Variable(tf.zeros([inp_dim]), dtype=tf.float32)\n",
    "        \n",
    "        self.W_velocity = tf.Variable(tf.zeros([inp_dim, hid_dim]), dtype=tf.float32, trainable=False)\n",
    "        self.bh_velocity = tf.Variable(tf.zeros([hid_dim]), dtype=tf.float32, trainable=False)\n",
    "        self.bv_velocity = tf.Variable(tf.zeros([inp_dim]), dtype=tf.float32, trainable=False)\n",
    "        \n",
    "        self.opt = tf.keras.optimizers.Adam(lr)\n",
    "    \n",
    "    @tf.function\n",
    "    def sample_hid(self, v):\n",
    "        v_cast = tf.cast(v, self.mixed_precision_dtype)\n",
    "        logits = tf.matmul(v_cast, tf.cast(self.W, self.mixed_precision_dtype)) + tf.cast(self.bh, self.mixed_precision_dtype)\n",
    "        h_prob = tf.nn.sigmoid(logits)\n",
    "        random_tensor = tf.random.uniform(tf.shape(h_prob), dtype=self.mixed_precision_dtype)\n",
    "        h_sample = tf.cast(h_prob > random_tensor, self.mixed_precision_dtype)\n",
    "        return tf.cast(h_prob, tf.float32), tf.cast(h_sample, tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def sample_vis(self, h):\n",
    "        h_cast = tf.cast(h, self.mixed_precision_dtype)\n",
    "        logits = tf.matmul(h_cast, tf.transpose(tf.cast(self.W, self.mixed_precision_dtype))) + tf.cast(self.bv, self.mixed_precision_dtype)\n",
    "        v_prob = tf.nn.sigmoid(logits)\n",
    "        random_tensor = tf.random.uniform(tf.shape(v_prob), dtype=self.mixed_precision_dtype)\n",
    "        v_sample = tf.cast(v_prob > random_tensor, self.mixed_precision_dtype)\n",
    "        return tf.cast(v_prob, tf.float32), tf.cast(v_sample, tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def contrastive_divergence_step(self, v0, k=1):\n",
    "    # Ensure consistent dtype for input\n",
    "        v0 = tf.cast(v0, tf.float32)\n",
    "    \n",
    "        h0_prob, h0_sample = self.sample_hid(v0)\n",
    "        vk_sample = v0  # This will now be float32\n",
    "        hk_sample = h0_sample\n",
    "    \n",
    "        for _ in tf.range(k):\n",
    "            vk_prob, vk_sample = self.sample_vis(hk_sample)\n",
    "            hk_prob, hk_sample = self.sample_hid(vk_sample)\n",
    "        \n",
    "        batch_size = tf.cast(tf.shape(v0)[0], tf.float32)\n",
    "        \n",
    "        pos_grad = tf.matmul(tf.transpose(v0), h0_prob)\n",
    "        neg_grad = tf.matmul(tf.transpose(vk_sample), hk_prob)\n",
    "        \n",
    "        w_grad = (pos_grad - neg_grad) / batch_size\n",
    "        bh_grad = tf.reduce_mean(h0_prob - hk_prob, axis=0)\n",
    "        bv_grad = tf.reduce_mean(v0 - vk_sample, axis=0)\n",
    "        \n",
    "        lr = self.opt.learning_rate\n",
    "        self.W_velocity.assign(self.momentum * self.W_velocity + lr * w_grad)\n",
    "        self.bh_velocity.assign(self.momentum * self.bh_velocity + lr * bh_grad)\n",
    "        self.bv_velocity.assign(self.momentum * self.bv_velocity + lr * bv_grad)\n",
    "        \n",
    "        self.W.assign_add(self.W_velocity)\n",
    "        self.bh.assign_add(self.bh_velocity)\n",
    "        self.bv.assign_add(self.bv_velocity)\n",
    "    \n",
    "        error = tf.reduce_mean(tf.square(v0 - vk_prob))\n",
    "        return error    \n",
    "    def train(self, data, epochs=10, batch_size=256, k=1, progress_bar=True):\n",
    "        dataset = (tf.data.Dataset.from_tensor_slices(data)\n",
    "                   .cache()\n",
    "                   .shuffle(10000)\n",
    "                   .batch(batch_size, drop_remainder=True)\n",
    "                   .prefetch(tf.data.AUTOTUNE))\n",
    "        \n",
    "        num_batches = len(list(dataset.as_numpy_iterator()))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if progress_bar:\n",
    "                batch_iter = tqdm(dataset, total=num_batches, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "            else:\n",
    "                batch_iter = dataset\n",
    "                \n",
    "            total_error = 0.0\n",
    "            \n",
    "            for batch in batch_iter:\n",
    "                error = self.contrastive_divergence_step(batch, k=k)\n",
    "                total_error += error\n",
    "            \n",
    "            avg_error = total_error / num_batches\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Avg Error: {avg_error:.6f}\")\n",
    "    \n",
    "    @tf.function\n",
    "    def transform(self, data):\n",
    "        v_cast = tf.cast(data, self.mixed_precision_dtype)\n",
    "        logits = tf.matmul(v_cast, tf.cast(self.W, self.mixed_precision_dtype)) + tf.cast(self.bh, self.mixed_precision_dtype)\n",
    "        h_prob = tf.nn.sigmoid(logits)\n",
    "        return tf.cast(h_prob, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastDBN:\n",
    "    def __init__(self, layer_dims, learning_rate=0.01, momentum=0.9, use_fp16=False):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.rbm_layers = []\n",
    "        self.use_fp16 = use_fp16\n",
    "        \n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            self.rbm_layers.append(\n",
    "                FastRBM(layer_dims[i], layer_dims[i + 1], \n",
    "                       lr=learning_rate, momentum=momentum, use_fp16=use_fp16)\n",
    "            )\n",
    "    \n",
    "    def pretrain(self, data, epochs_per_layer=10, batch_size=256, k=1, progress_bar=True):\n",
    "        print(\"Starting high-speed greedy layer-wise pretraining...\")\n",
    "        \n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "        else:\n",
    "            data = tf.cast(data, tf.float32)\n",
    "        \n",
    "        input_data = data\n",
    "        \n",
    "        for i, rbm in enumerate(self.rbm_layers):\n",
    "            print(f\"\\nPretraining layer {i+1}/{len(self.rbm_layers)}...\")\n",
    "            layer_start_time = tf.timestamp()\n",
    "            \n",
    "            rbm.train(input_data, epochs=epochs_per_layer, \n",
    "                     batch_size=batch_size, k=k, progress_bar=progress_bar)\n",
    "            \n",
    "            if tf.shape(input_data)[0] > 10000:\n",
    "                chunk_size = 10000\n",
    "                num_chunks = tf.shape(input_data)[0] // chunk_size + 1\n",
    "                \n",
    "                transformed_chunks = []\n",
    "                for j in range(num_chunks):\n",
    "                    start_idx = j * chunk_size\n",
    "                    end_idx = tf.minimum(start_idx + chunk_size, tf.shape(input_data)[0])\n",
    "                    if end_idx > start_idx:\n",
    "                        chunk = input_data[start_idx:end_idx]\n",
    "                        transformed_chunks.append(rbm.transform(chunk))\n",
    "                \n",
    "                input_data = tf.concat(transformed_chunks, axis=0)\n",
    "            else:\n",
    "                input_data = rbm.transform(input_data)\n",
    "            \n",
    "            elapsed = tf.timestamp() - layer_start_time\n",
    "            print(f\"Layer {i+1} training completed in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    def transform(self, data, in_chunks=True):\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "            \n",
    "        input_data = data\n",
    "        \n",
    "        if in_chunks and tf.shape(input_data)[0] > 10000:\n",
    "            chunk_size = 10000\n",
    "            \n",
    "            for rbm in self.rbm_layers:\n",
    "                transformed_chunks = []\n",
    "                \n",
    "                num_chunks = tf.shape(input_data)[0] // chunk_size + 1\n",
    "                for j in range(num_chunks):\n",
    "                    start_idx = j * chunk_size\n",
    "                    end_idx = tf.minimum(start_idx + chunk_size, tf.shape(input_data)[0])\n",
    "                    if end_idx > start_idx:\n",
    "                        chunk = input_data[start_idx:end_idx]\n",
    "                        transformed_chunks.append(rbm.transform(chunk))\n",
    "                \n",
    "                input_data = tf.concat(transformed_chunks, axis=0)\n",
    "        else:\n",
    "            for rbm in self.rbm_layers:\n",
    "                input_data = rbm.transform(input_data)\n",
    "                \n",
    "        return input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743683260.100709    2005 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20974 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:35:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting high-speed greedy layer-wise pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 12:27:41.805898: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1655760152 exceeds 10% of free system memory.\n",
      "2025-04-03 12:27:42.842868: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 827880076 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pretraining layer 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 12:27:43.257604: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 827880076 exceeds 10% of free system memory.\n",
      "2025-04-03 12:28:01.453828: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Epoch 1/5:  99%|█████████▉| 11829/11889 [00:16<00:00, 847.34it/s]2025-04-03 12:28:18.296635: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Epoch 1/5: 100%|██████████| 11889/11889 [00:16<00:00, 706.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Error: 5.009373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 11889/11889 [00:16<00:00, 742.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Error: 5.009662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|█████████▉| 11833/11889 [00:15<00:00, 838.56it/s]2025-04-03 12:28:50.247656: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Epoch 3/5: 100%|██████████| 11889/11889 [00:15<00:00, 746.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Error: 5.009740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 11889/11889 [00:15<00:00, 747.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Error: 5.009862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 11889/11889 [00:15<00:00, 747.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Avg Error: 5.009913\n",
      "Layer 1 training completed in 101.55 seconds\n",
      "\n",
      "Pretraining layer 2/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 12:29:24.453600: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6233449984 exceeds 10% of free system memory.\n",
      "2025-04-03 12:29:27.564698: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6233449984 exceeds 10% of free system memory.\n",
      "Epoch 1/5:  99%|█████████▉| 11821/11889 [00:16<00:00, 747.90it/s]2025-04-03 12:30:06.006329: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Epoch 1/5: 100%|██████████| 11889/11889 [00:16<00:00, 715.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Error: 0.001330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 11889/11889 [00:16<00:00, 720.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Error: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 11889/11889 [00:16<00:00, 717.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Error: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 11889/11889 [00:16<00:00, 715.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Error: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 11889/11889 [00:16<00:00, 717.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Avg Error: 0.000000\n",
      "Layer 2 training completed in 110.30 seconds\n",
      "\n",
      "Pretraining layer 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 11889/11889 [00:16<00:00, 740.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Error: 0.000325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 11889/11889 [00:15<00:00, 750.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Error: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|█████████▉| 11877/11889 [00:15<00:00, 786.16it/s]2025-04-03 12:32:22.724913: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Epoch 3/5: 100%|██████████| 11889/11889 [00:15<00:00, 747.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Error: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 11889/11889 [00:15<00:00, 746.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Error: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 11889/11889 [00:15<00:00, 748.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Avg Error: 0.000000\n",
      "Layer 3 training completed in 102.09 seconds\n"
     ]
    }
   ],
   "source": [
    "input_dim = x.shape[1]\n",
    "layer_dims = [input_dim, 128, 64, 32]\n",
    "    \n",
    "    # Create and pretrain the DBN\n",
    "dbn = FastDBN(layer_dims, learning_rate=0.01, momentum=0.9, use_fp16=True)\n",
    "dbn.pretrain(x, epochs_per_layer=5, batch_size=1024, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained DBN weights using manual saving\n",
    "for i, rbm in enumerate(dbn.rbm_layers):\n",
    "    weights_dict = {\n",
    "        'W': rbm.W.numpy(),\n",
    "        'bh': rbm.bh.numpy(),\n",
    "        'bv': rbm.bv.numpy()\n",
    "    }\n",
    "    np.savez(f\"rbm_layer_{i+1}_weights.npz\", **weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_x = dbn.transform(x.values)\n",
    "np.save(\"transformed_features.npy\", transformed_x)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
