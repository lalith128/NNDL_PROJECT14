{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 11:10:06.827198: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-03 11:10:06.914262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743678606.949355   23405 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743678606.959469   23405 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743678607.031614   23405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743678607.031748   23405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743678607.031751   23405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743678607.031753   23405 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-03 11:10:07.045638: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23405/2678278497.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv(\"../DATA/train.csv\")\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../DATA/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Protocol'] = pd.to_numeric(train['Protocol'], errors='coerce') \n",
    "train = train.dropna(subset=['Protocol']) \n",
    "train['Protocol'] = train['Protocol'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(train.columns) \n",
    "columns = [col for col in columns if col not in ['Fwd PSH Flags', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'Protocol', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.drop(columns=['Label'])\n",
    "y = train['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[columns] = x[columns].apply(pd.to_numeric, errors='coerce')\n",
    "f=['Fwd PSH Flags', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt']\n",
    "x[f] = x[f].apply(pd.to_numeric, errors='coerce')\n",
    "x = x.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = joblib.load('../FEATURE_EXTRACTION/scaler.pkl')  \n",
    "x[columns] = sc.transform(x[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "ye = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, ye, test_size=0.15, random_state=42, stratify=ye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(y_train)\n",
    "total_samples = np.sum(class_counts)\n",
    "class_weights = {i: total_samples / (len(class_counts) * count) for i, count in enumerate(class_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn_weights = [\n",
    "                np.load(\"../FEATURE_EXTRACTION/rbm_layer_1_weights.npz\"),\n",
    "                np.load(\"../FEATURE_EXTRACTION/rbm_layer_2_weights.npz\"),\n",
    "                np.load(\"../FEATURE_EXTRACTION/rbm_layer_3_weights.npz\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_high_accuracy_mlp(input_dim, num_classes, dbn_weights=None):\n",
    "    \"\"\"Build MLP model with optional DBN weight initialization\"\"\"\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Layer 1: Initial layer with DBN weights if available\n",
    "    x = Dense(512, activation='swish',\n",
    "                 kernel_initializer=tf.constant_initializer(dbn_weights[0]['W']),\n",
    "                 bias_initializer=tf.constant_initializer(dbn_weights[0]['bh']),\n",
    "                 kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(inputs)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # Layer 2\n",
    "    x = Dense(256, activation='swish', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Layer 3\n",
    "    x = Dense(128, activation='swish', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Layer 4\n",
    "    x = Dense(64, activation='swish', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Eager execution of tf.constant with unsupported shape. Tensor [[-4.9986500e-02 -3.0140501e-02 -1.6280431e-01 ... -4.0832300e-02\n  -9.9839736e-03  4.7602769e-02]\n [ 4.6088340e+04  4.6085285e+04  4.6086820e+04 ...  4.6087000e+04\n   4.6088379e+04  4.6088492e+04]\n [-7.9239684e-01 -7.2697359e-01 -7.2565013e-01 ... -1.1235443e+00\n  -7.2513372e-01 -6.6785139e-01]\n ...\n [ 1.8563789e-01  2.0050332e-02  1.4230958e-01 ... -1.0456318e-01\n   5.8039881e-02  1.4489168e-01]\n [ 1.7153993e-01 -8.6636402e-02  4.2609505e-02 ...  1.8159348e-01\n   5.4826528e-02 -1.3454488e-01]\n [-2.0515776e+00 -1.9022062e+00 -2.1558654e+00 ... -1.9476743e+00\n  -2.0807083e+00 -2.1908538e+00]] (converted from [[-4.9986500e-02 -3.0140501e-02 -1.6280431e-01 ... -4.0832300e-02\n  -9.9839736e-03  4.7602769e-02]\n [ 4.6088340e+04  4.6085285e+04  4.6086820e+04 ...  4.6087000e+04\n   4.6088379e+04  4.6088492e+04]\n [-7.9239684e-01 -7.2697359e-01 -7.2565013e-01 ... -1.1235443e+00\n  -7.2513372e-01 -6.6785139e-01]\n ...\n [ 1.8563789e-01  2.0050332e-02  1.4230958e-01 ... -1.0456318e-01\n   5.8039881e-02  1.4489168e-01]\n [ 1.7153993e-01 -8.6636402e-02  4.2609505e-02 ...  1.8159348e-01\n   5.4826528e-02 -1.3454488e-01]\n [-2.0515776e+00 -1.9022062e+00 -2.1558654e+00 ... -1.9476743e+00\n  -2.0807083e+00 -2.1908538e+00]]) has 2176 elements, but got `shape` (17, 512) with 8704 elements).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_high_accuracy_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbn_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mbuild_high_accuracy_mlp\u001b[0;34m(input_dim, num_classes, dbn_weights)\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(input_dim,))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Layer 1: Initial layer with DBN weights if available\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mswish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m             \u001b[49m\u001b[43mkernel_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant_initializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbn_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m             \u001b[49m\u001b[43mbias_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant_initializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbn_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mkernel_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml1_l2\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m BatchNormalization()(x)\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.5\u001b[39m)(x)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:48\u001b[0m, in \u001b[0;36mVariable._initialize_with_initializer.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_initialize_with_initializer\u001b[39m(\u001b[38;5;28mself\u001b[39m, initializer):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43minitializer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Eager execution of tf.constant with unsupported shape. Tensor [[-4.9986500e-02 -3.0140501e-02 -1.6280431e-01 ... -4.0832300e-02\n  -9.9839736e-03  4.7602769e-02]\n [ 4.6088340e+04  4.6085285e+04  4.6086820e+04 ...  4.6087000e+04\n   4.6088379e+04  4.6088492e+04]\n [-7.9239684e-01 -7.2697359e-01 -7.2565013e-01 ... -1.1235443e+00\n  -7.2513372e-01 -6.6785139e-01]\n ...\n [ 1.8563789e-01  2.0050332e-02  1.4230958e-01 ... -1.0456318e-01\n   5.8039881e-02  1.4489168e-01]\n [ 1.7153993e-01 -8.6636402e-02  4.2609505e-02 ...  1.8159348e-01\n   5.4826528e-02 -1.3454488e-01]\n [-2.0515776e+00 -1.9022062e+00 -2.1558654e+00 ... -1.9476743e+00\n  -2.0807083e+00 -2.1908538e+00]] (converted from [[-4.9986500e-02 -3.0140501e-02 -1.6280431e-01 ... -4.0832300e-02\n  -9.9839736e-03  4.7602769e-02]\n [ 4.6088340e+04  4.6085285e+04  4.6086820e+04 ...  4.6087000e+04\n   4.6088379e+04  4.6088492e+04]\n [-7.9239684e-01 -7.2697359e-01 -7.2565013e-01 ... -1.1235443e+00\n  -7.2513372e-01 -6.6785139e-01]\n ...\n [ 1.8563789e-01  2.0050332e-02  1.4230958e-01 ... -1.0456318e-01\n   5.8039881e-02  1.4489168e-01]\n [ 1.7153993e-01 -8.6636402e-02  4.2609505e-02 ...  1.8159348e-01\n   5.4826528e-02 -1.3454488e-01]\n [-2.0515776e+00 -1.9022062e+00 -2.1558654e+00 ... -1.9476743e+00\n  -2.0807083e+00 -2.1908538e+00]]) has 2176 elements, but got `shape` (17, 512) with 8704 elements)."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = build_high_accuracy_mlp(X_train.shape[1], len(np.unique(y)), dbn_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_boosted_accuracy(model, X_train, y_train, X_val, y_val, class_weights):\n",
    "    \"\"\"4-phase training strategy for optimal accuracy\"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True, mode='max', verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=10, min_lr=1e-7, mode='max', verbose=1),\n",
    "        ModelCheckpoint('best_mlp_model.keras', monitor='val_accuracy', save_best_only=True, mode='max'),\n",
    "        TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== Starting 4-Phase Training ===\")\n",
    "    \n",
    "    # Phase 1: Warmup (train only top layers)\n",
    "    for layer in model.layers[:-6]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    print(\"\\nPhase 1: Warmup Training (Top Layers Only)\")\n",
    "    history1 = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=2048,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Phase 2: Intermediate (unfreeze middle layers)\n",
    "    for layer in model.layers[:-10]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=5e-4),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    print(\"\\nPhase 2: Intermediate Training (Middle Layers)\")\n",
    "    history2 = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=60,\n",
    "        batch_size=1024,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Phase 3: Full Training (all layers)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    model.compile(optimizer=Nadam(learning_rate=1e-4),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    print(\"\\nPhase 3: Full Network Training\")\n",
    "    history3 = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=80,\n",
    "        batch_size=512,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Phase 4: Fine-tuning (very low LR)\n",
    "    model.compile(optimizer=Nadam(learning_rate=1e-5),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    print(\"\\nPhase 4: Fine-Tuning\")\n",
    "    history4 = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=256,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, (history1, history2, history3, history4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model, histories = train_with_boosted_accuracy(\n",
    "            model, X_train, y_train, X_test, y_test, class_weights\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
