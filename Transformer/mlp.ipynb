{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 04:49:23.119268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743655763.198938    1989 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743655763.225223    1989 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743655763.408932    1989 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743655763.409133    1989 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743655763.409137    1989 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743655763.409162    1989 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-03 04:49:23.440566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1989/2678278497.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv(\"../DATA/train.csv\")\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../DATA/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Protocol'] = pd.to_numeric(train['Protocol'], errors='coerce') \n",
    "train = train.dropna(subset=['Protocol']) \n",
    "train['Protocol'] = train['Protocol'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(train.columns) \n",
    "columns = [col for col in columns if col not in ['Fwd PSH Flags', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'Protocol', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.drop(columns=['Label'])\n",
    "y = train['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[columns] = x[columns].apply(pd.to_numeric, errors='coerce')\n",
    "f=['Fwd PSH Flags', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt']\n",
    "x[f] = x[f].apply(pd.to_numeric, errors='coerce')\n",
    "x = x.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = joblib.load('../FEATURE_EXTRACTION/scaler.pkl')  \n",
    "x[columns] = sc.transform(x[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "ye = le.fit_transform(y)\n",
    "class_counts = np.bincount(ye)\n",
    "class_weights = {i: 1./count for i, count in enumerate(class_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, ye, test_size=0.2, random_state=42,stratify=ye)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dbn_weights(layer_files):\n",
    "    weights = []\n",
    "    for file in layer_files:\n",
    "        data = np.load(file)\n",
    "        weights.append({\n",
    "            'W': data['W'],\n",
    "            'bh': data['bh'],\n",
    "            'bv': data['bv']\n",
    "        })\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn_weights = load_dbn_weights([\n",
    "    \"../FEATURE_EXTRACTION/rbm_layer_1_weights.npz\",\n",
    "    \"../FEATURE_EXTRACTION/rbm_layer_2_weights.npz\",\n",
    "    \"../FEATURE_EXTRACTION/rbm_layer_3_weights.npz\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_dim, num_classes, dbn_weights):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # DBN-initialized layers with trainable options\n",
    "    x = Dense(128, activation='swish',  # Swish often outperforms sigmoid/relu\n",
    "              kernel_initializer=tf.constant_initializer(dbn_weights[0]['W']),\n",
    "              bias_initializer=tf.constant_initializer(dbn_weights[0]['bh']),\n",
    "              kernel_regularizer=l2(1e-4))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(64, activation='swish',\n",
    "              kernel_initializer=tf.constant_initializer(dbn_weights[1]['W']),\n",
    "              bias_initializer=tf.constant_initializer(dbn_weights[1]['bh']),\n",
    "              kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(32, activation='swish',\n",
    "              kernel_initializer=tf.constant_initializer(dbn_weights[2]['W']),\n",
    "              bias_initializer=tf.constant_initializer(dbn_weights[2]['bh']),\n",
    "              kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Additional task-specific layers\n",
    "    x = Dense(64, activation='swish', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(32, activation='swish', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743655855.428843    1989 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20933 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "model = build_mlp(x_train.shape[1], num_classes, dbn_weights)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=True),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        mode='max'\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting two-phase training...\n",
      "Phase 1: Training new layers only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743655864.360011    4394 service.cc:152] XLA service 0x7f11cc018a80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743655864.360086    4394 service.cc:160]   StreamExecutor device (0): NVIDIA A10G, Compute Capability 8.6\n",
      "2025-04-03 04:51:04.459267: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1743655864.840410    4394 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  20/4756\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.0649 - loss: 0.0430   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743655868.252169    4394 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0475 - loss: 0.0064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 4ms/step - accuracy: 0.0475 - loss: 0.0064 - val_accuracy: 0.0286 - val_loss: 2.7240 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0395 - loss: 4.4027e-06 - val_accuracy: 1.1212e-04 - val_loss: 2.5512 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0848 - loss: 3.7848e-06 - val_accuracy: 1.1212e-04 - val_loss: 2.7347 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0844 - loss: 3.9616e-06 - val_accuracy: 0.0027 - val_loss: 2.6971 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.0736 - loss: 4.8912e-06 - val_accuracy: 0.0178 - val_loss: 2.4603 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.0777 - loss: 6.2873e-06 - val_accuracy: 0.0177 - val_loss: 2.6886 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m4739/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0905 - loss: 4.4745e-06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0905 - loss: 4.4737e-06 - val_accuracy: 0.5614 - val_loss: 2.3056 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0734 - loss: 3.9549e-06 - val_accuracy: 1.3347e-04 - val_loss: 2.3923 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0469 - loss: 5.3678e-06 - val_accuracy: 0.0068 - val_loss: 2.3156 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0497 - loss: 3.5614e-06 - val_accuracy: 0.0356 - val_loss: 2.5770 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.0453 - loss: 5.3037e-06 - val_accuracy: 0.0177 - val_loss: 2.8566 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.0815 - loss: 6.5481e-06 - val_accuracy: 1.3429e-04 - val_loss: 2.3773 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0610 - loss: 4.9220e-06 - val_accuracy: 0.0032 - val_loss: 2.4399 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0583 - loss: 5.4073e-06 - val_accuracy: 0.0177 - val_loss: 2.7481 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0362 - loss: 4.6221e-06 - val_accuracy: 0.0117 - val_loss: 2.6128 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.0371 - loss: 4.3813e-06 - val_accuracy: 0.0264 - val_loss: 7.6821 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.0424 - loss: 4.6749e-06 - val_accuracy: 0.0120 - val_loss: 2.6960 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0383 - loss: 4.1114e-06 - val_accuracy: 0.0120 - val_loss: 2.5722 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0322 - loss: 3.9164e-06 - val_accuracy: 0.0356 - val_loss: 2.5032 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.0376 - loss: 3.8994e-06 - val_accuracy: 0.0121 - val_loss: 2.6347 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.0405 - loss: 3.8903e-06 - val_accuracy: 1.1294e-04 - val_loss: 2.5198 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m4756/4756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.0427 - loss: 3.7817e-06 - val_accuracy: 0.0266 - val_loss: 2.4649 - learning_rate: 2.5000e-04\n",
      "\n",
      "Phase 2: Fine-tuning all layers\n",
      "Epoch 1/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3ms/step - accuracy: 0.0692 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 2/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0445 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0575 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0626 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0822 - loss: 27186436.0000 - val_accuracy: 1.3347e-04 - val_loss: 27186228.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0707 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0559 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0485 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0487 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 5.0000e-06\n",
      "Epoch 10/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0468 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 5.0000e-06\n",
      "Epoch 11/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0473 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 5.0000e-06\n",
      "Epoch 12/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0474 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 5.0000e-06\n",
      "Epoch 13/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.0446 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 5.0000e-06\n",
      "Epoch 14/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0388 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 5.0000e-06\n",
      "Epoch 15/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0409 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 5.0000e-06\n",
      "Epoch 16/100\n",
      "\u001b[1m9512/9512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.0384 - loss: 27186436.0000 - val_accuracy: 1.1992e-04 - val_loss: 27186228.0000 - learning_rate: 2.5000e-06\n",
      "Epoch 17/100\n",
      "\u001b[1m2517/9512\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.0412 - loss: 27186844.0000"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting two-phase training...\")\n",
    "\n",
    "# Phase 1: Train only the new layers (freeze DBN layers)\n",
    "for layer in model.layers[:6]:  # First 6 layers are DBN-initialized\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"Phase 1: Training new layers only\")\n",
    "history1 = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=2048,  # Larger batch size for stability\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Phase 2: Fine-tune all layers with lower learning rate\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 2: Fine-tuning all layers\")\n",
    "history2 = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=1024,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
